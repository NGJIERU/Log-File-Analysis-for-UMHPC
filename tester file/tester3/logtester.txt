[2022-09-22T14:01:55.589] _slurm_rpc_submit_batch_job: JobId=49477 InitPrio=15333 usec=693
[2022-09-22T14:01:55.692] sched: Allocate JobId=49477 NodeList=cpu07 #CPUs=32 Partition=cpu-opteron
[2022-09-22T14:01:56.125] error: slurmd error running JobId=49477 on node(s)=cpu07: Slurmd could not execve job
[2022-09-22T14:01:56.125] drain_nodes: node cpu07 state set to DRAIN
[2022-09-22T14:01:56.822] Requeuing JobId=49477
[2022-09-22T14:03:50.459] update_node: node cpu07 state set to IDLE
[2022-09-22T14:03:50.856] Node cpu07 now responding
[2022-09-22T14:04:06.412] _slurm_rpc_requeue: Requeue of JobId=49477 returned an error: Job is pending execution
[2022-09-22T14:04:14.397] sched: _release_job_rec: release hold on JobId=49477 by uid 0
[2022-09-22T14:04:14.397] _slurm_rpc_update_job: complete JobId=49477 uid=0 usec=984
[2022-09-22T14:04:14.881] sched: Allocate JobId=49477 NodeList=cpu07 #CPUs=32 Partition=cpu-opteron
[2022-09-22T14:04:15.087] error: slurmd error running JobId=49477 on node(s)=cpu07: Slurmd could not execve job
[2022-09-22T14:04:15.087] drain_nodes: node cpu07 state set to DRAIN
[2022-09-22T14:04:15.870] Requeuing JobId=49477
[2022-09-22T14:05:50.707] Recovered JobId=49477 Assoc=180
[2022-09-22T14:05:50.707] error: Aborting JobId=49477 due to change in socket/core configuration of allocated nodes

[2022-09-23T18:45:00.391] _slurm_rpc_submit_batch_job: JobId=49564 InitPrio=4222 usec=728
[2022-09-23T18:45:01.207] sched: Allocate JobId=49564 NodeList=cpu15 #CPUs=48 Partition=cpu-epyc
[2022-09-23T23:52:44.666] requeue job JobId=49564 due to failure of node cpu15
[2022-09-23T23:52:44.666] Requeuing JobId=49564
[2022-09-23T23:58:55.070] sched/backfill: _start_job: Started JobId=49564 in cpu-epyc on cpu153] error: slurm_receive_msgs: [[cpu15.dicc.um.edu.my]:6818] failed: Socket timed out on send/recv operation
[2022-09-23T23:59:05.349] _job_complete: JobId=49564 WEXITSTATUS 0
[2022-09-23T23:59:05.349] _job_complete: requeue JobId=49564 per user/system request
[2022-09-23T23:59:05.349] _job_complete: JobId=49564 done
[2022-09-24T00:04:57.019] Resending TERMINATE_JOB request JobId=49564 Nodelist=cpu15
[2022-09-24T00:57:08.790] _slurm_rpc_requeue: Requeue of JobId=49564 returned an error: Job is pending execution
[2022-09-24T12:13:05.003] _slurm_rpc_requeue: Requeue of JobId=49564 returned an error: Job is pending execution
[2022-09-24T12:19:27.192] cleanup_completing: JobId=49564 completion process took 44390 seconds
[2022-09-24T12:19:49.243] sched: Allocate JobId=49564 NodeList=cpu12 #CPUs=48 Partition=cpu-epyc
[2022-10-05T09:18:34.513] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=49564 uid 548200003

[2022-06-15T00:56:09.339] _slurm_rpc_submit_batch_job: JobId=43801 InitPrio=20010 usec=350
[2022-06-15T12:39:47.504] _slurm_rpc_requeue: Requeue of JobId=43801 returned an error: Job is pending execution
[2022-06-15T12:39:50.636] sched: Allocate JobId=43801 NodeList=cpu14 #CPUs=32 Partition=cpu-epyc
[2022-06-15T12:39:52.877] sched: _hold_job_rec: hold on JobId=43801 by uid 548200003
[2022-06-15T12:39:52.877] sched: _update_job: set priority to 0 for JobId=43801
[2022-06-15T12:39:52.877] _update_job: updating accounting
[2022-06-15T12:39:52.877] _slurm_rpc_update_job: complete JobId=43801 uid=548200003 usec=913
[2022-06-15T12:40:01.369] Requeuing JobId=43801
[2022-06-15T14:31:06.299] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=43801 uid 548300516

[2022-08-11T14:11:19.873] _slurm_rpc_submit_batch_job: JobId=47842 InitPrio=26699 usec=416
[2022-08-11T15:32:21.666] _slurm_rpc_requeue: Requeue of JobId=47842 returned an error: Job is pending execution
[2022-08-11T15:32:26.713] sched: Allocate JobId=47842 NodeList=gpu02 #CPUs=6 Partition=gpu-titan
[2022-08-11T15:32:37.498] Requeuing JobId=47842
[2022-08-11T15:53:29.590] _slurm_rpc_kill_job: REQUEST_KILL_JOB JobId=47842 uid 548300580
